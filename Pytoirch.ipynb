{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from Data_loading import FASHION_MNIST, cifar10, train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import io\n",
    "import gzip\n",
    "def FASHION_MNIST(flatten=True, one_hot=False):\n",
    "    base = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "    get  = lambda name: gzip.decompress(urllib.request.urlopen(base+name).read())\n",
    "\n",
    "    Xtr = np.frombuffer(get(\"train-images-idx3-ubyte.gz\"), dtype=np.uint8, offset=16).reshape(-1, 28, 28) / 255.0\n",
    "    ytr = np.frombuffer(get(\"train-labels-idx1-ubyte.gz\"), dtype=np.uint8, offset=8)\n",
    "\n",
    "    Xte = np.frombuffer(get(\"t10k-images-idx3-ubyte.gz\"), dtype=np.uint8, offset=16).reshape(-1, 28, 28) / 255.0\n",
    "    yte = np.frombuffer(get(\"t10k-labels-idx1-ubyte.gz\"), dtype=np.uint8, offset=8)\n",
    "\n",
    "    if flatten:\n",
    "        Xtr = Xtr.reshape(len(Xtr), -1)\n",
    "        Xte = Xte.reshape(len(Xte), -1)\n",
    "\n",
    "    if one_hot:\n",
    "        Ytr = np.zeros((ytr.size, 10))\n",
    "        Yte = np.zeros((yte.size, 10))\n",
    "        Ytr[np.arange(ytr.size), ytr] = 1\n",
    "        Yte[np.arange(yte.size), yte] = 1\n",
    "        return (Xtr, Ytr), (Xte, Yte)\n",
    "\n",
    "    return (Xtr, ytr), (Xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(Xtr, ytr), (x_test, y_test) = FASHION_MNIST(flatten=True, one_hot=False)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(Xtr, ytr, test_size=5000, random_state=42, shuffle=True)\n",
    "\n",
    "#x_train, y_train, x_valid, y_valid = train_val_split(Xtr, ytr, val_size=5000, seed=42)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.copy())\n",
    "y_train = torch.from_numpy(y_train.copy())\n",
    "\n",
    "x_valid = torch.from_numpy(x_valid.copy())\n",
    "y_valid = torch.from_numpy(y_valid.copy())\n",
    "\n",
    "x_test = torch.from_numpy(x_test.copy())\n",
    "y_test = torch.from_numpy(y_test.copy())\n",
    "\n",
    "x_train = x_train.float()\n",
    "x_valid = x_valid.float()\n",
    "x_test  = x_test.float()\n",
    "\n",
    "y_train = y_train.long()\n",
    "y_valid = y_valid.long()\n",
    "y_test  = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_l1 = 512\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "class FNNP(nn.Module):\n",
    "    def __init__(self,num_features,num_hidden_1,num_hidden_2,num_output):\n",
    "        super(FNNP,self).__init__()\n",
    "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(num_hidden_1, num_features)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden_1), 0))\n",
    "        # hidden layer 1\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(num_hidden_2, num_hidden_1)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_hidden_2), 0))\n",
    "\n",
    "        # hidden layer 2\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(num_hidden_2, num_hidden_1)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_hidden_2), 0))\n",
    "\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x,self.W_2,self.b_2)\n",
    "        return x\n",
    "network = FNNP(num_features, num_l1, num_classes)\n",
    "network = network.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter(init.xavier_normal_(torch.Tensor(1,2), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter(init.constant_(torch.Tensor(2), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have done this ourselves,\n",
    "# but we should be aware of sklearn and its tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 400\n",
    "num_epochs = 150\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    network.train()\n",
    "    for i in range(num_batches_train):\n",
    "        optimizer.zero_grad()\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = network(x_train[slce])\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = y_train[slce]\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss   \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    network.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = network(x_train[slce])\n",
    "        \n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(y_train[slce].numpy())\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        \n",
    "        output = network(x_valid[slce])\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_targs += list(y_valid[slce].numpy())\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        \n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
