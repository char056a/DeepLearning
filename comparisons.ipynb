{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7c7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from init import he_normal\n",
    "from Actfunc import Relu\n",
    "from Loss import*\n",
    "from FFNN import FFNN, Layer \n",
    "from Data_loading import FASHION_MNIST, train_val_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86e4c1",
   "metadata": {},
   "source": [
    "## Here we will show a simple usage of our FFNN class and compare it to a pytorch network with the same parameters when trained on Fashion-MNIST. For details on the FFNN class, see the file FFNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53a80d",
   "metadata": {},
   "source": [
    "### We set up helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea298312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_one_hot(y, num_classes=10):\n",
    "    # returns (num_classes, batch_size)\n",
    "    oh = np.zeros((num_classes, y.size))\n",
    "    oh[y, np.arange(y.size)] = 1.0\n",
    "    return oh\n",
    "\n",
    "\n",
    "def accuracy(logits, y_true):\n",
    "    preds = np.argmax(logits, axis=0)\n",
    "    return np.mean(preds == y_true)\n",
    "\n",
    "dataset=\"Fashion-MNIST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206208f6",
   "metadata": {},
   "source": [
    "### Set up parameters for the network and training (here we use Adam as our optimizer). The parameters used are not important, we just want to show that our implementation will give the same results as pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650cf6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes=[200,200]\n",
    "init_fn=he_normal\n",
    "act_fn=Relu\n",
    "beta=0.6\n",
    "gamma=0.4\n",
    "epochs = 3\n",
    "batch_size = 1000\n",
    "lr = 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913d38c",
   "metadata": {},
   "source": [
    "### Load the data (note that we are treating every observation as a column-vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81940945",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtr, ytr), (Xte, yte) = FASHION_MNIST(flatten=True, one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde96bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split(Xtr, ytr, val_size=5000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ebed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[0]\n",
    "output_size = 10\n",
    "N_train = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1ea63",
   "metadata": {},
   "source": [
    "### Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3765b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FFNN(input_size, hidden_sizes, output_size, init_fn, act_fn, beta, gamma,lambda_=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddc11f",
   "metadata": {},
   "source": [
    "### We now train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74fe4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 1\n",
      "Epoch 1/3  Train acc: 0.6728  Val acc: 0.6646\n",
      "epoch number 2\n",
      "Epoch 2/3  Train acc: 0.7622  Val acc: 0.7570\n",
      "epoch number 3\n",
      "Epoch 3/3  Train acc: 0.7956  Val acc: 0.7942\n",
      "FINAL TEST  -  loss: 0.6232  acc: 0.7863\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f\"epoch number {epoch+1}\")\n",
    "\n",
    "\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "    perm = np.random.permutation(N_train)\n",
    "    y_train_oh = to_one_hot(y_train, output_size)\n",
    "    y_val_oh   = to_one_hot(y_val, output_size)\n",
    "    y_test_oh  = to_one_hot(yte, output_size)\n",
    "\n",
    "\n",
    "    for start in range(0, N_train, batch_size):\n",
    "        idx = perm[start:start+batch_size]\n",
    "        X_batch = X_train[:, idx]\n",
    "        y_batch = y_train[idx]\n",
    "\n",
    "        y_batch_oh = y_train_oh[:, idx]\n",
    "\n",
    "        logits, A, Z = net.forward(X_batch)\n",
    "        loss = cross_entropy_batch(y_batch_oh, logits)\n",
    "\n",
    "        \n",
    "\n",
    "        epoch_train_loss += loss\n",
    "        num_batches += 1\n",
    "\n",
    "        grads_w, grads_b = net.full_gradient(A, Z, y_batch_oh, X_batch,lambda_=0.0)\n",
    "        net.update_wb(grads_w, grads_b, learning_rate=lr, Adam=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    epoch_train_loss /= num_batches\n",
    "\n",
    "    # ---- Eval on train + val ----\n",
    "    train_logits, _, _ = net.forward(X_train)\n",
    "    val_logits, _, _   = net.forward(X_val)\n",
    "\n",
    "    train_oh = to_one_hot(y_train, output_size)\n",
    "    val_oh   = to_one_hot(y_val,   output_size)\n",
    "\n",
    "    train_loss = cross_entropy_batch(train_oh, train_logits)\n",
    "    val_loss   = cross_entropy_batch(val_oh,   val_logits)\n",
    "\n",
    "    train_acc = accuracy(train_logits, y_train)\n",
    "    val_acc   = accuracy(val_logits, y_val)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}  \"\n",
    "            f\"Train acc: {train_acc:.4f}  Val acc: {val_acc:.4f}\")\n",
    "\n",
    "# ---- Final test evaluation (only once, after training) ----\n",
    "test_logits, _, _ = net.forward(Xte)\n",
    "test_oh = to_one_hot(yte, output_size)\n",
    "\n",
    "test_loss = cross_entropy_batch(test_oh, test_logits)\n",
    "test_acc  = accuracy(test_logits, yte)\n",
    "\n",
    " \n",
    "\n",
    "print(f\"FINAL TEST  -  loss: {test_loss:.4f}  acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ddf34",
   "metadata": {},
   "source": [
    "# Pytorch with same parameters (copied from exercise sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8e2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a35d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import io\n",
    "import gzip\n",
    "def FASHION_MNIST(flatten=True, one_hot=False):\n",
    "    base = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "    get  = lambda name: gzip.decompress(urllib.request.urlopen(base+name).read())\n",
    "\n",
    "    Xtr = np.frombuffer(get(\"train-images-idx3-ubyte.gz\"), dtype=np.uint8, offset=16).reshape(-1, 28, 28) / 255.0\n",
    "    ytr = np.frombuffer(get(\"train-labels-idx1-ubyte.gz\"), dtype=np.uint8, offset=8)\n",
    "\n",
    "    Xte = np.frombuffer(get(\"t10k-images-idx3-ubyte.gz\"), dtype=np.uint8, offset=16).reshape(-1, 28, 28) / 255.0\n",
    "    yte = np.frombuffer(get(\"t10k-labels-idx1-ubyte.gz\"), dtype=np.uint8, offset=8)\n",
    "\n",
    "    if flatten:\n",
    "        Xtr = Xtr.reshape(len(Xtr), -1)\n",
    "        Xte = Xte.reshape(len(Xte), -1)\n",
    "\n",
    "    if one_hot:\n",
    "        Ytr = np.zeros((ytr.size, 10))\n",
    "        Yte = np.zeros((yte.size, 10))\n",
    "        Ytr[np.arange(ytr.size), ytr] = 1\n",
    "        Yte[np.arange(yte.size), yte] = 1\n",
    "        return (Xtr, Ytr), (Xte, Yte)\n",
    "\n",
    "    return (Xtr, ytr), (Xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495bb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(Xtr, ytr), (x_test, y_test) = FASHION_MNIST(flatten=True, one_hot=False)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(Xtr, ytr, test_size=5000, random_state=42, shuffle=True)\n",
    "\n",
    "#x_train, y_train, x_valid, y_valid = train_val_split(Xtr, ytr, val_size=5000, seed=42)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.copy())\n",
    "y_train = torch.from_numpy(y_train.copy())\n",
    "\n",
    "x_valid = torch.from_numpy(x_valid.copy())\n",
    "y_valid = torch.from_numpy(y_valid.copy())\n",
    "\n",
    "x_test = torch.from_numpy(x_test.copy())\n",
    "y_test = torch.from_numpy(y_test.copy())\n",
    "\n",
    "x_train = x_train.float()\n",
    "x_valid = x_valid.float()\n",
    "x_test  = x_test.float()\n",
    "\n",
    "y_train = y_train.long()\n",
    "y_valid = y_valid.long()\n",
    "y_test  = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac9c21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_l1 = hidden_sizes[0]\n",
    "num_l2 = hidden_sizes[1]\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "class FNNP(nn.Module):\n",
    "    def __init__(self,num_features,num_hidden_1,num_hidden_2,num_output):\n",
    "        super(FNNP,self).__init__()\n",
    "        self.W_1 = Parameter(init.kaiming_normal_(torch.Tensor(num_hidden_1, num_features)))\n",
    "        self.b_1 = Parameter(torch.reshape(init.kaiming_normal_(torch.Tensor(num_hidden_1,1)),(-1,))) \n",
    "        # hidden layer 1\n",
    "        self.W_2 = Parameter(init.kaiming_normal_(torch.Tensor(num_hidden_2, num_hidden_1)))\n",
    "        self.b_2 = Parameter(torch.reshape(init.kaiming_normal_(torch.Tensor(num_hidden_2,1)),(-1,))) \n",
    "\n",
    "        # hidden layer 2\n",
    "        self.W_3 = Parameter(init.kaiming_normal_(torch.Tensor(num_output, num_hidden_2)))\n",
    "        self.b_3 = Parameter(torch.reshape(init.kaiming_normal_(torch.Tensor(num_output,1)),(-1,))) \n",
    "\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x,self.W_2,self.b_2)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x,self.W_3,self.b_3)\n",
    "        return x\n",
    "network = FNNP(num_features, num_l1,num_l2, num_classes)\n",
    "network = network.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26d2e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=lr,betas=(beta,gamma))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e81efbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : Train Loss 0.122161 , Train acc 0.631745, Valid acc 0.632600\n",
      "Epoch  2 : Train Loss 0.050827 , Train acc 0.750764, Valid acc 0.753800\n",
      "Epoch  3 : Train Loss 0.036697 , Train acc 0.788873, Valid acc 0.788000\n",
      "\n",
      "Final Test Accuracy: 0.7797\n"
     ]
    }
   ],
   "source": [
    "# we could have done this ourselves,\n",
    "# but we should be aware of sklearn and its tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = batch_size\n",
    "num_epochs = epochs\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    network.train()\n",
    "    for i in range(num_batches_train):\n",
    "        optimizer.zero_grad()\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = network(x_train[slce])\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = y_train[slce]\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss   \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    network.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = network(x_train[slce])\n",
    "        \n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(y_train[slce].numpy())\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        \n",
    "        output = network(x_valid[slce])\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_targs += list(y_valid[slce].numpy())\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        \n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    preds = torch.max(output, 1)[1]\n",
    "    test_acc_final = accuracy_score(y_test.numpy(), preds.numpy())\n",
    "\n",
    "print(\"\\nFinal Test Accuracy: {:.4f}\".format(test_acc_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af4032",
   "metadata": {},
   "source": [
    "### As can be seen, the accuracies are very similar, indicating that our implementation is very close to the results of pytorch. feel free to vary beta, gamma and the learning rate and try again. Note that the pytorch part assumes only 2 hidden layers. The size of each of these two layers can be varied with the hidden_sizes parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
