{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d76b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7470a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\n",
      "6.5\n",
      "Softmax:\n",
      "[5.60279546e-09 1.52299771e-08 9.99999825e-01 4.13993700e-08\n",
      " 1.12535155e-07]\n",
      "Cross:\n",
      "1.747673127945547e-07\n"
     ]
    }
   ],
   "source": [
    "from Actfunc import*\n",
    "from init import*\n",
    "from Loss import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d55a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relu(np.array([-2,0,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db3645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self,dim_in,dim_out,init_fn,act_fn=Relu):\n",
    "        self.weights=init_fn(dim_in,dim_out)\n",
    "        self.bias=np.reshape(init_fn(1,dim_out),-1)\n",
    "        self.act_fn=act_fn\n",
    "        self.weights_m = np.zeros_like(self.weights)\n",
    "        self.weights_v =np.zeros_like(self.weights)\n",
    "        self.bias_m = np.zeros_like(self.bias)\n",
    "        self.bias_v = np.zeros_like(self.bias)\n",
    "\n",
    "\n",
    "    def fw(self,input):\n",
    "        output=self.weights@input + self.bias[:,None]\n",
    "        Z=output\n",
    "        if self.act_fn:\n",
    "            output=self.act_fn(Z)\n",
    "        return output,Z\n",
    "\n",
    "        \n",
    "\n",
    "    def gradient_last(self,onehot,A,Z):\n",
    "        S=softmax_matrix(Z[-1])\n",
    "        delta=S-onehot\n",
    "        d_WL=np.einsum('ij,sj->jis',delta,A[-2]) # i stedet for d_WL=np.outer(delta,A[-2])\n",
    "        d_BL=delta\n",
    "        return d_WL, d_BL\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2597237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "\n",
    "    def __init__(self,input_size,hidden_sizes,output_size, init_fn,act_fn, beta, gamma):\n",
    "        self.layers=[]\n",
    "        self.layers.append(Layer(dim_in=input_size,dim_out=hidden_sizes[0],init_fn=init_fn,act_fn=act_fn))\n",
    "        for i in range(1,len(hidden_sizes)):\n",
    "            self.layers.append(Layer(dim_in=hidden_sizes[i-1],dim_out=hidden_sizes[i],init_fn=init_fn,act_fn=act_fn))\n",
    "        self.layers.append(Layer(dim_in=hidden_sizes[-1],dim_out=output_size,init_fn=init_fn,act_fn=False))\n",
    "        self.t = 0\n",
    "        self.beta =beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        A=[]\n",
    "        Z=[]\n",
    "        output=input\n",
    "        for layer in self.layers:\n",
    "            a,z=layer.fw(output)\n",
    "            A.append(a)\n",
    "            Z.append(z)\n",
    "            output=a\n",
    "        return output,A,Z\n",
    "    \n",
    "    def full_gradient(self,A,Z,onehot,input):\n",
    "        gradients_w=[]\n",
    "        gradients_b=[]\n",
    "        dLi_dW,dLi_dB=self.layers[-1].gradient_last(onehot,A,Z)\n",
    "        dLi_df=dLi_dB\n",
    "        gradients_w.insert(0,np.sum(dLi_dW,axis=0))\n",
    "        gradients_b.insert(0,np.sum(dLi_dB,axis=1))\n",
    "        for i in range(len(self.layers)-2,0,-1):\n",
    "            dLi_df=((self.layers[i+1].weights.T@dLi_df)*(Z[i]>0)) #\n",
    "            dLi_dB=dLi_df #\n",
    "            dLi_dW=np.einsum('ij,sj->jis',dLi_df,A[i-1]) # instead of dLi_dW=np.outer(dLi_df,A[i-1])\n",
    "            gradients_w.insert(0,np.sum(dLi_dW,axis=0))\n",
    "            gradients_b.insert(0,np.sum(dLi_dB,axis=1))\n",
    "        dLi_df=((self.layers[1].weights.T@dLi_df)*(Z[0]>0)) #\n",
    "        dLi_dB=dLi_df\n",
    "        dLi_dW=np.einsum('ij,sj->jis',dLi_df,input)# instead of dLi_dW=np.outer(dLi_df,input)\n",
    "        gradients_w.insert(0,np.sum(dLi_dW,axis=0))\n",
    "        gradients_b.insert(0,np.sum(dLi_dB,axis=1))\n",
    "        return gradients_w,gradients_b\n",
    "    \n",
    "    def update_wb(self,gradients_w,gradients_b,learning_rate, Adam=False):\n",
    "        if Adam:\n",
    "            self.t +=1\n",
    "            t = self.t\n",
    "            for layer, weights, biases in zip(self.layers, gradients_w, gradients_b):\n",
    "                layer.weight_m = self.beta * layer.weight_m + (1-self.beta) * weights\n",
    "                layer.weitgh_v = self.gamma * layer.weight_v + (1-self.gamma) * (weights)**2\n",
    "                m_tilde_w = layer.weight_m/(1-self.beta**t)\n",
    "                v_tilde_w = layer.weight_v/(1-self.gamma**t)\n",
    "\n",
    "                layer.weights -= learning_rate * m_tilde_w/(np.sqrt(v_tilde_w)+10**(-7))\n",
    "\n",
    "                layer.bias_m = self.beta * layer.bias_m + (1-self.beta) * biases\n",
    "                layer.bias_v = self.gamma * layer.bias_v + (1-self.gamma) * (biases)**2\n",
    "                m_tilde_b = layer.bias_m/(1-self.beta**t)\n",
    "                v_tilde_b = layer.bias_v/(1-self.gamma**t)\n",
    "\n",
    "                layer.bias -= learning_rate * m_tilde_b/(np.sqrt(v_tilde_b) + 10**(-7))\n",
    "        else:\n",
    "            for layer,weights,biases in zip(self.layers,gradients_w,gradients_b):\n",
    "                layer.weights -= learning_rate * weights\n",
    "                layer.bias -= learning_rate * biases\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f190f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=FFNN(input_size=3,hidden_sizes=[2,3,6,7,60,70,80,90,100,90,80,20],output_size=5,init_fn=he_normal,act_fn=Relu)\n",
    "test_input=np.random.randint(low=1,high=10,size=(3,6000))\n",
    "test_onehots=np.zeros(shape=(5,6000))\n",
    "test_onehots[0,:]=np.ones(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f6cb3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output,A,Z=test.forward(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8e81e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DW,DB=test.full_gradient(A,Z,test_onehots,test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
