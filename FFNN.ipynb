{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d76b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7470a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\n",
      "6.5\n",
      "Softmax:\n",
      "[5.60279546e-09 1.52299771e-08 9.99999825e-01 4.13993700e-08\n",
      " 1.12535155e-07]\n",
      "Cross:\n",
      "1.747673127945547e-07\n"
     ]
    }
   ],
   "source": [
    "from Actfunc import*\n",
    "from init import*\n",
    "from Loss import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d55a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relu(np.array([-2,0,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db3645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self,dim_in,dim_out,init_fn,act_fn=Relu):\n",
    "        self.weights=init_fn(dim_in,dim_out)\n",
    "        self.bias=np.reshape(init_fn(1,dim_out),-1)\n",
    "        self.act_fn=act_fn\n",
    "\n",
    "\n",
    "    def fw(self,input):\n",
    "        output=self.weights@input + self.bias[:,None]\n",
    "        Z=output\n",
    "        if self.act_fn:\n",
    "            output=self.act_fn(Z)\n",
    "        return output,Z\n",
    "\n",
    "        \n",
    "\n",
    "    def gradient_last(self,onehot,A,Z):\n",
    "        S=softmax_matrix(Z[-1])\n",
    "        delta=S-onehot\n",
    "        d_WL=np.einsum('ij,sj->jis',delta,A[-2]) # i stedet for d_WL=np.outer(delta,A[-2])\n",
    "        d_BL=delta\n",
    "        return d_WL, d_BL\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2597237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "\n",
    "    def __init__(self,input_size,hidden_sizes,output_size, init_fn,act_fn):\n",
    "        self.layers=[]\n",
    "        self.layers.append(Layer(dim_in=input_size,dim_out=hidden_sizes[0],init_fn=init_fn,act_fn=act_fn))\n",
    "        for i in range(1,len(hidden_sizes)):\n",
    "            self.layers.append(Layer(dim_in=hidden_sizes[i-1],dim_out=hidden_sizes[i],init_fn=init_fn,act_fn=act_fn))\n",
    "        self.layers.append(Layer(dim_in=hidden_sizes[-1],dim_out=output_size,init_fn=init_fn,act_fn=False))\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        A=[]\n",
    "        Z=[]\n",
    "        output=input\n",
    "        for layer in self.layers:\n",
    "            a,z=layer.fw(output)\n",
    "            A.append(a)\n",
    "            Z.append(z)\n",
    "            output=a\n",
    "        return output,A,Z\n",
    "    \n",
    "    def full_gradient(self,A,Z,onehot,input):\n",
    "        gradients_w=[]\n",
    "        gradients_b=[]\n",
    "        dLi_dW,dLi_dB=self.layers[-1].gradient_last(onehot,A,Z)\n",
    "        dLi_df=dLi_dB\n",
    "        gradients_w.insert(0,np.sum(dLi_dW,axis=0))\n",
    "        gradients_b.insert(0,np.sum(dLi_dB,axis=1))\n",
    "        for i in range(len(self.layers)-2,0,-1):\n",
    "            dLi_df=((self.layers[i+1].weights.T@dLi_df)*(Z[i]>0)) #\n",
    "            dLi_dB=dLi_df #\n",
    "            dLi_dW=np.einsum('ij,sj->jis',dLi_df,A[i-1]) # instead of dLi_dW=np.outer(dLi_df,A[i-1])\n",
    "            gradients_w.insert(0,np.sum(dLi_dW,axis=0))\n",
    "            gradients_b.insert(0,np.sum(dLi_dB,axis=1))\n",
    "        dLi_df=((self.layers[1].weights.T@dLi_df)*(Z[0]>0)) #\n",
    "        dLi_dB=dLi_df\n",
    "        dLi_dW=np.einsum('ij,sj->jis',dLi_df,input)# instead of dLi_dW=np.outer(dLi_df,input)\n",
    "        gradients_w.insert(0,np.sum(dLi_dW,axis=0))\n",
    "        gradients_b.insert(0,np.sum(dLi_dB,axis=1))\n",
    "        return gradients_w,gradients_b\n",
    "    \n",
    "    def update_wb(self,gradients_w,gradients_b,learning_rate):\n",
    "        for layer, weights, biases in zip(self.layers, gradients_w, gradients_b):\n",
    "            layer.weights -= learning_rate * weights\n",
    "            layer.bias -= learning_rate * biases\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f190f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=FFNN(input_size=3,hidden_sizes=[2,3,6,7,60,70,80,90,100,90,80,20],output_size=5,init_fn=he_normal,act_fn=Relu)\n",
    "test_input=np.random.randint(low=1,high=10,size=(3,6000))\n",
    "test_onehots=np.zeros(shape=(5,6000))\n",
    "test_onehots[0,:]=np.ones(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f6cb3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output,A,Z=test.forward(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8e81e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DW,DB=test.full_gradient(A,Z,test_onehots,test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57685702",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbefore = [layers.weights.copy() for layers in test.layers]\n",
    "\n",
    "test.update_wb(DW,DB,learning_rate=0.01)\n",
    "\n",
    "Wafter = [layers.weights.copy() for layers in test.layers]\n",
    "\n",
    "wbefore[0]-Wafter[0] #det giver non zero entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2269e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100\n",
    "num_epochs = 200\n",
    "\n",
    "num_sample_train = 300 #x_train.shape[]\n",
    "num_batches = num_sample_train // batch_size\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epochloss = 0\n",
    "    for i in range(num_batches):\n",
    "        # definer x_batch og y_batch\n",
    "        x_batch = 100 # just a number\n",
    "        y_batch = 100 # jsut a numper\n",
    "\n",
    "        output,A,Z  = test.forward(x_batch)\n",
    "        loss = cross_entropy_single(y_batch,output)\n",
    "        epochloss += loss\n",
    "\n",
    "        # finder gradienter\n",
    "        gW,gB = test.full_gradient(A,Z,y_batch,x_batch)\n",
    "        test.update_wb(gW,gB, learning_rate=0.01)\n",
    "    train_loss.append(epochloss/num_batches)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
